<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Anti-Spoof Live</title>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
<style>
  body { text-align:center; font-family:Arial; }

  .video-wrapper {
    position: relative;
    width: min(400px, 92vw);
    height: min(400px, 92vw);
    margin: 0 auto;
  }

  video {
    width: 100%;
    height: 100%;
    object-fit: cover;
  }

  /* Oval mask */
  .video-wrapper::after {
    content: "";
    position: absolute;
    inset: 0;
    pointer-events: none;

    /* Dark overlay with transparent oval cutout */
    background:
      radial-gradient(
        ellipse 50% 80% at center,
        transparent 0%,
        transparent 60%,
        rgba(0,0,0,0.65) 61%
      );
  }

  #switchBtn {
    margin-top:8px;
    padding:6px 14px;
    font-size:14px;
    cursor:pointer;
  }
video {
  width: 100%;
  height: 100%;
  object-fit: cover;
  transform: scaleX(-1);   /* mirror preview only */
}

</style>

</head>
<body>

<h2 style="font-size:30px; margin-bottom:6px;">Live Anti-Spoof</h2>

<p style="font-size:15px; color:#444; max-width:360px; margin:8px auto 16px; line-height:1.4;">
  • Ensure your face is clearly visible in good lighting (avoid harsh reflections).<br>
  • Align and fit your face tightly within the oval.
</p>


<div class="video-wrapper">
  <video id="video" width="224" height="224" autoplay playsinline></video>
</div>

<canvas id="canvas" width="224" height="224" style="display:none;"></canvas>
<br>
<button id="switchBtn" onclick="switchCamera()">⟳ Switch Camera</button>
<p id="result">Loading model...</p>

<script>
let session;
let running = false;
let facingMode = "user";
let currentStream = null;

async function init() {
  session = await ort.InferenceSession.create("antispoof_model.onnx");
  document.getElementById("result").innerText = "Model loaded. Starting camera...";
  startCamera();
}

async function startCamera() {
  if (currentStream) {
    currentStream.getTracks().forEach(t => t.stop());
  }
  const video = document.getElementById("video");
  const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode } });
  currentStream = stream;
  video.srcObject = stream;
  running = true;
  runLoop();
}

async function switchCamera() {
  running = false;
  facingMode = facingMode === "user" ? "environment" : "user";
  await startCamera();
}

async function runLoop() {
  if (!running) return;

  const video = document.getElementById("video");
  const canvas = document.getElementById("canvas");
  const ctx = canvas.getContext("2d");

  ctx.drawImage(video, 0, 0, 224, 224);
  const imageData = ctx.getImageData(0, 0, 224, 224).data;

  const floatData = new Float32Array(1 * 3 * 224 * 224);

  const mean = [0.485, 0.456, 0.406];
  const std  = [0.229, 0.224, 0.225];

  for (let i = 0; i < 224 * 224; i++) {
    const r = imageData[i * 4] / 255;
    const g = imageData[i * 4 + 1] / 255;
    const b = imageData[i * 4 + 2] / 255;

    floatData[i] = (r - mean[0]) / std[0];
    floatData[i + 224*224] = (g - mean[1]) / std[1];
    floatData[i + 2*224*224] = (b - mean[2]) / std[2];
  }

  const tensor = new ort.Tensor("float32", floatData, [1,3,224,224]);

  const results = await session.run({ input: tensor });

  const logit = results.output.data[0];
  const prob = 1 / (1 + Math.exp(-logit));

  document.getElementById("result").innerText =
    prob > 0.5 ? `REAL (${prob.toFixed(3)})` :
                 `ATTACK (${prob.toFixed(3)})`;

  requestAnimationFrame(runLoop);
}

init();
</script>

</body>
</html>
