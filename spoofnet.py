# -*- coding: utf-8 -*-
"""SpoofNet

Automatically generated by Colab.
"""

# !pip install -q timm torch torchvision pillow matplotlib scikit-learn

"""imports"""

import os
import zipfile
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import timm
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, confusion_matrix
from tqdm import tqdm
import random

# Set seeds for reproducibility
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

"""Drive mount"""

from google.colab import drive
drive.mount('/content/drive')

# Update this path to your zip file location
ZIP_PATH = '/content/drive/MyDrive/MainData/FULL_DATASET_FRAMES.zip'
EXTRACT_PATH = '/content/dataset'

print("Extracting dataset...")
with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
    zip_ref.extractall(EXTRACT_PATH)
print("✓ Dataset extracted")

"""Dataset class"""

class AntispoofDataset(Dataset):
    def __init__(self, real_path, attack_path, transform=None):
        self.samples = []
        self.transform = transform

        for identity in os.listdir(real_path):
            identity_path = os.path.join(real_path, identity)
            if os.path.isdir(identity_path):
                for img_name in os.listdir(identity_path):
                    if img_name.endswith(('.jpg', '.png', '.jpeg')):
                        self.samples.append((os.path.join(identity_path, img_name), 1))

        for identity in os.listdir(attack_path):
            identity_path = os.path.join(attack_path, identity)
            if os.path.isdir(identity_path):
                for img_name in os.listdir(identity_path):
                    if img_name.endswith(('.jpg', '.png', '.jpeg')):
                        self.samples.append((os.path.join(identity_path, img_name), 0))

        print(f"Loaded {len(self.samples)} samples")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        image = Image.open(img_path).convert("RGB")

        if self.transform:
            image = self.transform(image)

        return image, torch.tensor(label, dtype=torch.float32)

"""Data augmentation"""

train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],
                         [0.229,0.224,0.225])
])

test_transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],
                         [0.229,0.224,0.225])
])

"""Dataset paths"""

TRAIN_REAL = os.path.join(EXTRACT_PATH, 'FULL_DATASET_FRAMES/train/real')
TRAIN_ATTACK = os.path.join(EXTRACT_PATH, 'FULL_DATASET_FRAMES/train/attack')
TEST_REAL = os.path.join(EXTRACT_PATH, 'FULL_DATASET_FRAMES/test/real')
TEST_ATTACK = os.path.join(EXTRACT_PATH, 'FULL_DATASET_FRAMES/test/attack')

train_dataset = AntispoofDataset(TRAIN_REAL, TRAIN_ATTACK, train_transform)
test_dataset = AntispoofDataset(TEST_REAL, TEST_ATTACK, test_transform)

train_loader = DataLoader(train_dataset, batch_size=64,
                          shuffle=True, num_workers=2, pin_memory=True)

test_loader = DataLoader(test_dataset, batch_size=64,
                         shuffle=False, num_workers=2, pin_memory=True)

"""CNN"""

class MobileNetTexture(nn.Module):
    def __init__(self):
        super().__init__()

        backbone = timm.create_model(
            'mobilenetv3_small_100',
            pretrained=True,
            features_only=True
        )

        self.backbone = backbone

        # Get feature dimensions
        feature_info = backbone.feature_info
        mid_channels = feature_info[2]['num_chs']   # mid-level
        final_channels = feature_info[-1]['num_chs']

        # Texture branch (from mid-level features)
        self.texture_branch = nn.Sequential(
            nn.Conv2d(mid_channels, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten()
        )

        self.global_pool = nn.AdaptiveAvgPool2d(1)

        self.classifier = nn.Sequential(
            nn.Linear(final_channels + 64, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        features = self.backbone(x)

        mid_feat = features[2]
        final_feat = features[-1]

        texture_feat = self.texture_branch(mid_feat)

        main_feat = self.global_pool(final_feat)
        main_feat = main_feat.view(main_feat.size(0), -1)

        combined = torch.cat([main_feat, texture_feat], dim=1)

        out = self.classifier(combined)
        return out

"""Model setup"""

model = MobileNetTexture().to(device)

criterion = nn.BCEWithLogitsLoss()

optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)

scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)

NUM_EPOCHS = 20
best_acc = 0

"""Training"""

for epoch in range(NUM_EPOCHS):

    model.train()
    train_loss = 0
    correct = 0
    total = 0

    for images, labels in tqdm(train_loader):
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(images).squeeze()
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        preds = (torch.sigmoid(outputs) > 0.5).float()
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    train_acc = 100 * correct / total

    # Validation
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in tqdm(test_loader):
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images).squeeze()
            loss = criterion(outputs, labels)

            test_loss += loss.item()

            probs = torch.sigmoid(outputs)
            preds = (probs > 0.5).float()

            correct += (preds == labels).sum().item()
            total += labels.size(0)

            all_preds.extend(probs.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    test_acc = 100 * correct / total
    auc = roc_auc_score(all_labels, all_preds)

    print(f"\nEpoch {epoch+1}/{NUM_EPOCHS}")
    print(f"Train Acc: {train_acc:.2f}%")
    print(f"Test Acc:  {test_acc:.2f}% | AUC: {auc:.4f}")

    if test_acc > best_acc:
        best_acc = test_acc
        torch.save(model.state_dict(), "/content/best_model.pth")
        print("✓ Best model saved")

    scheduler.step()

"""Rebuild and load"""

import torch
import torch.nn as nn
import timm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class MobileNetTexture(nn.Module):
    def __init__(self):
        super().__init__()

        backbone = timm.create_model(
            'mobilenetv3_small_100',
            pretrained=False,   # IMPORTANT: set False when loading weights
            features_only=True
        )

        self.backbone = backbone

        feature_info = backbone.feature_info
        mid_channels = feature_info[2]['num_chs']
        final_channels = feature_info[-1]['num_chs']

        self.texture_branch = nn.Sequential(
            nn.Conv2d(mid_channels, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten()
        )

        self.global_pool = nn.AdaptiveAvgPool2d(1)

        self.classifier = nn.Sequential(
            nn.Linear(final_channels + 64, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        features = self.backbone(x)

        mid_feat = features[2]
        final_feat = features[-1]

        texture_feat = self.texture_branch(mid_feat)

        main_feat = self.global_pool(final_feat)
        main_feat = main_feat.view(main_feat.size(0), -1)

        combined = torch.cat([main_feat, texture_feat], dim=1)

        out = self.classifier(combined)
        return out


# Instantiate
model = MobileNetTexture().to(device)

# Load weights
model.load_state_dict(torch.load("/content/best_model.pth", map_location=device))

model.eval()

print("✓ Model loaded successfully")

# forward pass
dummy_input = torch.randn(1, 3, 224, 224).to(device)

with torch.no_grad():
    output = model(dummy_input)

print("Output shape:", output.shape)
print("Raw output:", output)

"""Export"""

!pip install onnxruntime onnxscript

import torch
onnx_path = "/content/antispoof_model.onnx"

dummy_input = torch.randn(1, 3, 224, 224).to(device)

model.eval()

torch.onnx.export(
    model,
    dummy_input,
    "/content/antispoof_model.onnx",
    export_params=True,
    opset_version=13,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={
        'input': {0: 'batch_size'},
        'output': {0: 'batch_size'}
    },
    external_data=False   # ← THIS is the correct argument
)

print("Re-exported without external data.")

import onnxruntime as ort
import numpy as np

# Create same random input
dummy_input = torch.randn(1, 3, 224, 224).to(device)

# PyTorch output
with torch.no_grad():
    torch_output = model(dummy_input).cpu().numpy()

# ONNX output
ort_session = ort.InferenceSession(onnx_path)

ort_inputs = {
    ort_session.get_inputs()[0].name: dummy_input.cpu().numpy()
}

ort_output = ort_session.run(None, ort_inputs)[0]

print("PyTorch output:", torch_output)
print("ONNX output:   ", ort_output)

print("Difference:", np.abs(torch_output - ort_output))

from google.colab import files
files.download("/content/antispoof_model.onnx")
